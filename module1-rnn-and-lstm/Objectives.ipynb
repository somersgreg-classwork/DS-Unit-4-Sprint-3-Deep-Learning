{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 01 - Describe Neural Networks Used for Modeling Sequences\n",
    "## Overview\n",
    "We have reached the last sprint of the core data science curriculum! In this unit so far, we have created and trained feed-forward neural networks. While we can do a lot with this type of neural network, some types of data work better with different architectures.\n",
    "\n",
    "This module will explore recurrent neural networks (RNN), and a type of RNN called a long short-term memory (LSTM) network. These architectures are well suited for processing sequences and using them for many natural language processing tasks.\n",
    "\n",
    "### Sequence\n",
    "A sequence is a collection of objects (integers, floats, characters, tokens, and other data types) where you can repeat the order of matter and objects. A Python list is an example, as well as NumPy arrays. Many of the data structures we use are built on basic sequences.\n",
    "\n",
    "### Time Series\n",
    "A time series is a data where you have not just the order but some actual continuous marker for where the points lie “in time” - this could be a date, a timestamp, Unix time, or something else. Of course, all time series are also sequences, and for some techniques, you might consider the order of the sequence and not the separation (in time) of the entries.\n",
    "\n",
    "### Recursion\n",
    "In mathematics, recursion is defining objects based on previously defined other objects of the same type. In other words, recursion is something that happens when a thing calls itself one or more times.\n",
    "\n",
    "For example, a recursive function calls itself and uses its previous terms to define subsequent terms. Pascal’s Triangle https://en.wikipedia.org/wiki/Pascal%27s_triangle is an example of using previous terms to calculate subsequent terms: each number is the sum of the two numbers directly above it.\n",
    "\n",
    "In computer science, a recursive function calls itself from within its code.\n",
    "\n",
    "### Recurrent Neural Networks (RNN)\n",
    "Remember that a feed-forward neural network has an input layer and then some number of hidden layers. The output from each layer is fed into the next layer without any feedback. In contrast, with a recurrent neural network, there is a layer where the output from the nodes feeds back into itself. This layer is called the recurrent layer.\n",
    "\n",
    "Simple RNNs have a weakness called the vanishing gradient problem: the recursive aspect sometimes results in the back-propagation gradients either exploding or becoming very small (vanishing). So what can we do?\n",
    "\n",
    "### Long short-term memory (LSTM) network\n",
    "To prevent the vanishing gradient problem, we can create a memory state within the network that adds to the gradients; this prevents them from becoming too small. You can learn more about the structure of the LSTM network in this article. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/ For now, we’ll focus on how to implement them and what types of problems they are suitable for.\n",
    "\n",
    "## Follow Along\n",
    "In this section, we’ll first look at the option in Keras for creating a simple neural network with a recurrent layer. The keras.layers.SimpleRNN is a fully connected RNN where the output from the previous time step is fed to the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### USE THIS TO HIDE DEBUGGING LOGS\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               24704     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,994\n",
      "Trainable params: 89,994\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example: https://keras.io/guides/working_with_rnns/\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Instantiate the model\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "# Add an additional hidden layer\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "# View the architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output before using os.environ command\n",
    "\"\"\"\n",
    "2022-04-12 20:50:16.112964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
    "2022-04-12 20:50:16.113099: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, None, 64)          64000     \n",
    "                                                                 \n",
    " simple_rnn (SimpleRNN)      (None, 128)               24704     \n",
    "                                                                 \n",
    " dense (Dense)               (None, 10)                1290      \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 89,994\n",
    "Trainable params: 89,994\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "2022-04-12 20:50:19.510435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
    "2022-04-12 20:50:19.510488: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
    "2022-04-12 20:50:19.510529: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Opti17): /proc/driver/nvidia/version does not exist\n",
    "2022-04-12 20:50:19.511434: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
    "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can also create a network with a LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               98816     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,106\n",
      "Trainable params: 164,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Example: https://keras.io/guides/working_with_rnns/\n",
    "\n",
    "# LSTM network example\n",
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size 1000, and\n",
    "# output embedding dimension of size 64.\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dense layer with 10 units.\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge  \n",
    "Before class time, it would be good to review the Keras: Working with RNNs documentation. Ensure you know how to add a recurrent layer and the difference between a simple RNN and LSTM.\n",
    "\n",
    "## Additional Resources  \n",
    "Keras: Working with RNNs  https://keras.io/guides/working_with_rnns/  \n",
    "Recurrent Neural Networks: LSTM Tutorial https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective 02 - Apply an LSTM to a Text Generation Problem Using Keras\n",
    "## Overview\n",
    "In the first part of this module, we generally learned why recurrent neural networks are a good choice for working with sequential data, such as text. Now, we will implement a specific type of RNN called a long short-term memory network (LSTM) to make text predictions.\n",
    "\n",
    "LSTM networks are suitable for text prediction and generation because they can remember long sequences of data. So, let’s test out how to implement an LSTM network with text prediction.\n",
    "\n",
    "## Follow Along\n",
    "We’ll use text from Project Gutenberg https://www.gutenberg.org/ and use a portion of it to train the neural network. The novel is the Adventures of Sherlock Holmes by Arthur Conan Doyle; the shortened text used in the following analysis is also available here https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-practice-datasets/main/unit_4/sherlock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-practice-datasets/main/unit_4/sherlock.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Strip the \\r\\n characters\n",
    "text = text.replace('\\r\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a single string of text. However, the neural network input needs to be numeric, so we must convert or encode the text as characters. We can create two look-up tables: character to integer and integer to character (to make predictions after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the text: 91\n"
     ]
    }
   ],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Find the unique characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "print('The number of unique characters in the text:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create sequences of the characters to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  54974\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "# Encode the characters using the lookup tables\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "# Initialize empty lists to hold the sequences\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "# Loop through the entire text\n",
    "for i in range(0, len(encoded) - maxlen, step): \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "\n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that the text is processed, we can build our model! We’ll use a Keras utility to pad our sequences, so they are all the same length up to the maximum we specify. Then, we’ll create our feature and target arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Pad sequences so all are equal\n",
    "seq = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=40)\n",
    "\n",
    "# Create x & y\n",
    "import numpy as np\n",
    "\n",
    "# Create arrays of zeros (False)\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=bool)\n",
    "\n",
    "# Turn on the location (set to True) when the character is present\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "\n",
    "    y[i, next_char[i]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we will use has an input layer equal to the number of characters in our text, a hidden layer of 64 nodes, an LSTM layer of 64 nodes, and an output layer equal to the character set’s size. We are predicting one of the characters, so we need to reflect that in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model: a single LSTM\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Bidirectional, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=64, input_dim=len(chars)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s fit the model! We will choose a lower number of epochs for this text run because neural networks usually take some time to train. We can adjust the epochs later to see how our results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1718/1718 - 53s - loss: 2.5809 - 53s/epoch - 31ms/step\n",
      "Epoch 2/5\n",
      "1718/1718 - 50s - loss: 2.2156 - 50s/epoch - 29ms/step\n",
      "Epoch 3/5\n",
      "1718/1718 - 48s - loss: 2.0920 - 48s/epoch - 28ms/step\n",
      "Epoch 4/5\n",
      "1718/1718 - 49s - loss: 2.0058 - 49s/epoch - 28ms/step\n",
      "Epoch 5/5\n",
      "1718/1718 - 49s - loss: 1.9405 - 49s/epoch - 28ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15d8abfa60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(seq, y, batch_size=32,\n",
    "          epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit the model, we need to convert the numeric predictions back into characters, so that we can read it. We'll create a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and convert text back into characters\n",
    "def generate_text(model, seed, length):\n",
    "\n",
    "  encoded = [char_int[c] for c in seed]\n",
    "\n",
    "  generated = ''\n",
    "  generated += seed\n",
    "  model.reset_states()\n",
    "\n",
    "  start_index = 0 \n",
    "\n",
    "  for _ in range(length):\n",
    "\n",
    "      sample = encoded[start_index:start_index+10]      \n",
    "      sample = np.array(sample)\n",
    "      sample = np.expand_dims(sample,0)\n",
    "\n",
    "      pred = model.predict(sample)\n",
    "      pred = tf.squeeze(pred, 0)\n",
    "      next_char = np.argmax(pred)\n",
    "      encoded.append(next_char)\n",
    "      generated += int_char[next_char]\n",
    "\n",
    "      start_index += 1\n",
    "\n",
    "  return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theoriesmorhrtoa tn tn tnsosenenlaen ene ah the r ne teeore af  tes aorhrtn ert nee af  te eng ah thet etoree th thrdethe r nd ere yernahethetheeu   d yddtantdtneahe sotoathr t  an thhe hnn  y otneetot t  thhesntetnethe  hnhr  daheoheee  e sepootne toaeeded ed e dnsasotoooerdeeeeddedaou rr  ythhsndthee saootsodfhend  ethhhhee  eer er erareoaeosore n dddaansoeonk edth rdd d tnotncooen n unn   dddd  d nnnse'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the seed text which the model will use to generate the predicted text\n",
    "seed_text = \"I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theories\"\n",
    "\n",
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is interesting! We have something resembling language, but the words don’t make any sense - I don’t know what an “aootttd” is, but it could be exciting! There also isn’t any punctuation or other structure in the text. But, we only trained the network for five epochs, which isn’t very many.\n",
    "\n",
    "Let's increase that to 100 epochs and compare the output, using the same seed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1718/1718 [==============================] - 88s 51ms/step - loss: 1.0437\n",
      "Epoch 2/2\n",
      "1718/1718 [==============================] - 75s 44ms/step - loss: 1.0299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f157447dd30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 TAKES 80 MINS! CANT DO THIS!\n",
    "# Train with more epochs\n",
    "model.fit(seq, y, batch_size=32,\n",
    "          epochs=2, verbose=1)  # VERBOSE SET TO 0 IS VERY BAD!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"I have no data yet it is a capital mistake to theorise before one has data insensibly one begins to twist facts to suit theories\"\n",
    "\n",
    "generate_text(model, seed_text, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the text is starting to develop some structure, with punctuation and even a few words that seem more like words?\n",
    "\n",
    "We kept this example simple so that you could see how to set up an LSTM for generating text. Usually, you would use more layers to capture the structure of the text better.\n",
    "\n",
    "## Challenge  \n",
    "Now it is up to you! Using the exact text and code above, add additional layers to the network and see if you can improve the text prediction.\n",
    "\n",
    "You can even take it a step further and source a new text, load it, and process it in the same way, and see what your network can generate.\n",
    "\n",
    "## Additional Resources  \n",
    "Understanding LSTMs https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
    "Text Generation Using Python https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/  \n",
    "Text Generation With LSTM Recurrent Neural Networks in Python with Keras \n",
    "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd8e00f63139cf9feba2231e80738099b1c7a506243ed7a456c0669dfb5d242f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
