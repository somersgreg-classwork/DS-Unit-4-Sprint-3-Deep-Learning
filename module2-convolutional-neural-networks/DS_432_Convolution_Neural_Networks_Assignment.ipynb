{"cells":[{"cell_type":"markdown","metadata":{"id":"fc4yMj7mtCAZ"},"source":["\n","\n","## *Data Science Unit 4 Sprint 3 Assignment 2*\n","# Convolutional Neural Networks (CNNs)"]},{"cell_type":"markdown","metadata":{"id":"0lfZdD_cp1t5"},"source":["# Assignment\n","\n","- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n","- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n","- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n","\n","\n","You will apply three different CNN models to a binary image classification model using Keras. Classify images of mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n","\n","|Mountain (+)|Forest (-)|\n","|---|---|\n","|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/train/mountain/art1131.jpg?raw=1)|![](https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data/validation/forest/cdmc317.jpg?raw=1)|\n","\n","The problem is relatively difficult given that the sample is tiny: about 350 observations per class. However, this sample size might be something that you can expect when prototyping an image classification problem/solution at work Get accustomed to evaluating several different possible models."]},{"cell_type":"markdown","metadata":{"id":"1eawBP-otCAb"},"source":["# Pre-Trained Model\n","<a id=\"p1\"></a>\n","\n","Load a pre-trained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n","\n","```python\n","import numpy as np\n","\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n","\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model # This is the functional API\n","\n","resnet = ResNet50(weights='imagenet', include_top=False)\n","\n","```\n","\n","The `include_top` = False parameter in `ResNet50` will remove the fully connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n","\n","```python\n","for layer in resnet.layers:\n","    layer.trainable = False\n","```\n","\n","Using the Keras functional API, we will need to add additional full connected layers to our model. We removed the top layers, and we removed all previous fully connected layers. In other words, we kept only the feature processing portions of our network. The `GlobalAveragePooling2D` layer functions as a fancy flatten function by taking the average of each of the last convolutional layer outputs (two dimensional still). \n","\n","```python\n","x = resnet.output\n","x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n","x = Dense(1024, activation='relu')(x)\n","predictions = Dense(1, activation='sigmoid')(x)\n","model = Model(resnet.input, predictions)\n","```\n","\n","Your assignment is to apply the transfer learning above to classify images of mountains (`./data/train/mountain/*`) and images of forests (`./data/train/forest/*`). Treat mountains as the positive class (1) and the forest images as the negative (zero). \n","\n","Steps to complete assignment: \n","1. Load in Image Data into NumPy arrays (`X`) \n","2. Create a `y` for the labels\n","3. Train your model with pre-trained layers from resnet\n","4. Report your model's accuracy"]},{"cell_type":"markdown","metadata":{"id":"xHBOEp77_LRY"},"source":["-----\n","\n","# GPU on Colab\n","\n","If you're working on Colab, you only have access to 2 processors, so your model training will be slow. However, if you turn on the GPU instance that you have access to, your model training will be faster!\n","\n","[**Instructions for turning on GPU on Colab**](https://colab.research.google.com/notebooks/gpu.ipynb)\n","\n","------"]},{"cell_type":"markdown","metadata":{"id":"CLdGdXCatCAb"},"source":["## Load in Data\n","\n","Loading data is surprisingly more complicated than it seems because you are working with directories of images instead of a single file. \n","\n","This boilerplate will help you download a zipped version of the directory of images. The directory is organized into **train** and **validation** directories which you can use inside an `ImageGenerator` class to stream batches of images through your model.  \n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2922,"status":"ok","timestamp":1639522456550,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"PDYKkQNw_LRZ"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-04-15 21:40:01.051834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-04-15 21:40:01.051884: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]}],"source":["from os import listdir\n","from os.path import isfile, join\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","import os\n","\n","import numpy as np\n","\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n","\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model # This is the functional API\n","\n","from tensorflow.keras import datasets\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n","\n","from keras.preprocessing.image import array_to_img, img_to_array, load_img"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":175,"status":"ok","timestamp":1639522458601,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"WwgASoWQ_LRa"},"outputs":[],"source":["%matplotlib inline\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FZotV8NS_LRb"},"outputs":[],"source":["# Clear any tensorboard logs from previous runs\n","!rm -rf ./logs/"]},{"cell_type":"markdown","metadata":{"id":"moRVuHUqtCAc"},"source":["### Download & Summarize the Data\n","\n","This step is completed for you. Just run the cells and review the results. "]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":925,"status":"ok","timestamp":1639524053136,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"AR66H8o9tCAc"},"outputs":[],"source":["# # data url\n","# _URL = 'https://github.com/LambdaSchool/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/data.zip?raw=true'\n","\n","# # download data and save to `file_name`\n","# file_name = './data.zip'\n","# path_to_zip = tf.keras.utils.get_file(file_name, origin=_URL, extract=True,cache_dir='/content')\n","\n","\n","# # get absolute path to location of the data that we just downloaded\n","# PATH = os.path.join(os.path.dirname(path_to_zip), 'data')"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1639524056562,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"ius8bOYa_LRc","outputId":"8771c462-27e6-46a3-8625-61bac5d93d8b"},"outputs":[],"source":["# protip: go to your terminal and paste the output below and cd into it\n","# explore it a bit...we'll come back to this later - muahahaha!!!\n","PATH = \"./data\""]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1639524073210,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"0n7rYSYOKU8k","outputId":"e0536676-5784-4783-fffb-15f51d566414"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[34;42mtrain\u001b[0m/  \u001b[34;42mvalidation\u001b[0m/\n"]}],"source":["%ls ./data\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":124,"status":"ok","timestamp":1639522470157,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"MNFsIu_KtCAg"},"outputs":[],"source":["# create train data dir path\n","train_dir = os.path.join(PATH, 'train')\n","\n","# create validation data dir path\n","validation_dir = os.path.join(PATH, 'validation')"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":155,"status":"ok","timestamp":1639522472436,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"OsI9BQLotCAj"},"outputs":[],"source":["# train directory with mountian data sub-dir \n","train_mountain_dir = os.path.join(train_dir, 'mountain') \n","\n","# train directory with forest data sub-dir \n","train_forest_dir = os.path.join(train_dir, 'forest')  \n","\n","# validation directory with mountain data sub-dir \n","validation_mountain_dir = os.path.join(validation_dir, 'mountain')  \n","\n","# validation directory with forest data sub-dir \n","validation_forest_dir = os.path.join(validation_dir, 'forest')  "]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1639522475835,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"NUs1e5-XtCAl"},"outputs":[],"source":["# get the number of samples in each of the sub-dir \n","num_mountain_tr = len(os.listdir(train_mountain_dir))\n","num_forest_tr = len(os.listdir(train_forest_dir))\n","\n","num_mountain_val = len(os.listdir(validation_mountain_dir))\n","num_forest_val = len(os.listdir(validation_forest_dir))\n","\n","# get the total number of samples for the train and validation sets\n","total_train = num_mountain_tr + num_forest_tr\n","total_val = num_mountain_val + num_forest_val"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1639522478264,"user":{"displayName":"Joseph catanzarite","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmMRaw8NgoEDqzevEZ6b18iOYdEH9nWTZeaFBW=s64","userId":"16649206137414945374"},"user_tz":480},"id":"ZmklbgSMtCAn","outputId":"ca68bffc-8833-4b8a-dc9a-931c0c6191a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["total training mountain images: 252\n","total training forest images: 268\n","total validation mountain images: 123\n","total validation forest images: 60\n","--\n","Total training images: 520\n","Total validation images: 183\n"]}],"source":["print('total training mountain images:', num_mountain_tr)\n","print('total training forest images:', num_forest_tr)\n","\n","print('total validation mountain images:', num_mountain_val)\n","print('total validation forest images:', num_forest_val)\n","print(\"--\")\n","print(\"Total training images:\", total_train)\n","print(\"Total validation images:\", total_val)"]},{"cell_type":"markdown","metadata":{"id":"0QBSj-OR_LRe"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"dQ4ag4ultCAq"},"source":["### Use Keras `ImageDataGenerator` to Process the Data\n","\n","This step is completed for you, but please review the code. The `ImageDataGenerator` class reads in batches of data from a directory and passes them to the model one batch at a time. Like large text files, this method is advantageous because it stifles the need to load many images into memory.\n","\n","At this point you should review the documentation for the [Keras ImageDataGenerator Class](https://keras.io/preprocessing/image/#imagedatagenerator-class). <br>\n","You'll expand its use in the next section."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"67i9IW49tCAq"},"outputs":[],"source":["batch_size = 16\n","epochs = 10 # feel free to change this value only after you've gone through the notebook once  \n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"B1wNKMo1tCAt"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# ImageDataGenerator can rescale data from within \n","max_pixel_val = 255.\n","rescale = 1./max_pixel_val\n","\n","# Generator for our training data\n","train_image_generator = ImageDataGenerator(rescale=rescale) \n","                                           \n","# Generator for our validation data                                           \n","validation_image_generator = ImageDataGenerator(rescale=rescale) "]},{"cell_type":"code","execution_count":31,"metadata":{"id":"ndsuM4L9tCAv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 520 images belonging to 2 classes.\n"]}],"source":["# Takes the path to a directory and generates batches of augmented data\n","train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                           directory=train_dir,\n","                                                           shuffle=True,\n","                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                           class_mode='binary', \n","                                                           color_mode='rgb')"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"kOUTWqD2_LRf"},"outputs":[{"name":"stdout","output_type":"stream","text":["['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_filepaths', '_flow_index', '_get_batches_of_transformed_samples', '_keras_api_names', '_keras_api_names_v1', '_set_index_array', 'allowed_class_modes', 'batch_index', 'batch_size', 'class_indices', 'class_mode', 'classes', 'color_mode', 'data_format', 'directory', 'dtype', 'filenames', 'filepaths', 'image_data_generator', 'image_shape', 'index_array', 'index_generator', 'interpolation', 'labels', 'lock', 'n', 'next', 'num_classes', 'on_epoch_end', 'reset', 'sample_weight', 'samples', 'save_format', 'save_prefix', 'save_to_dir', 'seed', 'set_processing_attrs', 'shuffle', 'split', 'subset', 'target_size', 'total_batches_seen', 'white_list_formats']\n"]}],"source":["# explore some of `train_data_gen` attributes to get a sense of what this object can do for you\n","# YOUR CODE HERE\n","print(dir(train_data_gen))"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"9kxlk3optCAy"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 182 images belonging to 2 classes.\n"]}],"source":["# Takes the path to a directory & generates batches of augmented data\n","val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n","                                                              directory=validation_dir,\n","                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                              class_mode='binary', \n","                                                              color_mode='rgb')"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"uFUl9DQN_LRg"},"outputs":[{"name":"stdout","output_type":"stream","text":["['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_filepaths', '_flow_index', '_get_batches_of_transformed_samples', '_keras_api_names', '_keras_api_names_v1', '_set_index_array', 'allowed_class_modes', 'batch_index', 'batch_size', 'class_indices', 'class_mode', 'classes', 'color_mode', 'data_format', 'directory', 'dtype', 'filenames', 'filepaths', 'image_data_generator', 'image_shape', 'index_array', 'index_generator', 'interpolation', 'labels', 'lock', 'n', 'next', 'num_classes', 'on_epoch_end', 'reset', 'sample_weight', 'samples', 'save_format', 'save_prefix', 'save_to_dir', 'seed', 'set_processing_attrs', 'shuffle', 'split', 'subset', 'target_size', 'total_batches_seen', 'white_list_formats']\n"]}],"source":["# explore some of `val_data_gen` attributes to get a sense of what this object can do for you\n","# YOUR CODE HERE\n","print(dir(val_data_gen))"]},{"cell_type":"markdown","metadata":{"id":"2l7ue6NutCA0"},"source":["_____\n","## Instantiate Model\n","\n","Here your job is to take the python code at the beginning of the notebook (in the markdown cell) and turn it into working code. \n","\n","Most of the code that you'll need to build a model is in that markdown cell, though you'll still need to compile the model.\n","\n","Some pseudo-code is provided as a guide. "]},{"cell_type":"code","execution_count":37,"metadata":{"deletable":false,"id":"mKNIYOEItCA0","nbgrader":{"cell_type":"code","checksum":"3ef6aebc8089f2297504b310c583cb98","grade":false,"grade_id":"cell-f5f8bf566ce32a9b","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-04-15 22:07:40.794700: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","2022-04-15 22:07:40.794793: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n","2022-04-15 22:07:40.795013: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Opti17): /proc/driver/nvidia/version does not exist\n","2022-04-15 22:07:40.796339: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 45s 0us/step\n","94781440/94765736 [==============================] - 45s 0us/step\n"]}],"source":["# load in the pre-trained model\n","# YOUR CODE HERE\n","resnet = ResNet50(weights='imagenet', include_top=False)"]},{"cell_type":"markdown","metadata":{"id":"gyjAtLTkcQj4"},"source":["### \"Freeze\" all the pretrained resnet50 weights and biases \n","so they will *not* be updated by gradient descent and backpropagation during training. <br>\n","Your model will train only the additional dense layers the classification part that you add to it below. <br>This is the essence of **Transfer Learning**: adapting a pre-trained model to a new data set that is different from the data used to train the mode."]},{"cell_type":"code","execution_count":38,"metadata":{"deletable":false,"id":"C8BSyRjz_LRh","nbgrader":{"cell_type":"code","checksum":"16a5d78e4c810864427217d8846aa811","grade":false,"grade_id":"cell-c9ce632ff4dce6bc","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# YOUR CODE HERE\n","for layer in resnet.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":39,"metadata":{"deletable":false,"id":"SXwptGVm_LRh","nbgrader":{"cell_type":"code","checksum":"93c62210094db18dc989941d624caa9f","grade":false,"grade_id":"cell-9ab2a3f78406ccee","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# YOUR CODE HERE\n","# take the model output layer as a starting point\n","x = resnet.output\n","# take a global average pool\n","x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n","# add a trainable hidden layer with 1024 nodes \n","x = Dense(1024, activation='relu')(x)\n","# add a trainable output layer \n","predictions = Dense(1, activation='sigmoid')(x)\n","# put it all together using the Keras's Model api\n","model = Model(resnet.input, predictions)"]},{"cell_type":"code","execution_count":40,"metadata":{"deletable":false,"id":"YlMRjhkI_LRh","nbgrader":{"cell_type":"code","checksum":"4fb9a9608c3d9bc53d7d570ae4e9eaf9","grade":false,"grade_id":"cell-2f47a473b5842014","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# the only code that is missing is the compile method \n","\n","# YOUR CODE HERE\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # bin cro ent because its a binary classification task. Can use 'precision',recall,f1 score"]},{"cell_type":"markdown","metadata":{"id":"BVPBWYG7tCA2"},"source":["## Fit Model"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"9NpArCzp_LRh"},"outputs":[],"source":["# include the callback into the fit method\n","# we'll launch tensorboard in the last section\n","logdir = os.path.join(\"logs\", \"resnet_model\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"H4XdvWA5tCA3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","32/32 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.5813"]},{"name":"stderr","output_type":"stream","text":["2022-04-15 22:15:07.668777: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 566231040 exceeds 10% of free system memory.\n","2022-04-15 22:15:08.006639: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 503316480 exceeds 10% of free system memory.\n","2022-04-15 22:15:08.537028: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 566231040 exceeds 10% of free system memory.\n","2022-04-15 22:15:09.009245: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 566231040 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["32/32 [==============================] - 42s 1s/step - loss: 0.6823 - accuracy: 0.5813 - val_loss: 0.7664 - val_accuracy: 0.3409\n"]},{"name":"stderr","output_type":"stream","text":["2022-04-15 22:15:09.362015: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 503316480 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10\n","32/32 [==============================] - 43s 1s/step - loss: 0.5723 - accuracy: 0.7024 - val_loss: 0.5083 - val_accuracy: 0.8409\n","Epoch 3/10\n","32/32 [==============================] - 40s 1s/step - loss: 0.5251 - accuracy: 0.7520 - val_loss: 0.4667 - val_accuracy: 0.8352\n","Epoch 4/10\n","32/32 [==============================] - 39s 1s/step - loss: 0.4795 - accuracy: 0.8016 - val_loss: 0.4273 - val_accuracy: 0.7784\n","Epoch 5/10\n","32/32 [==============================] - 39s 1s/step - loss: 0.4251 - accuracy: 0.8413 - val_loss: 0.5897 - val_accuracy: 0.6761\n","Epoch 6/10\n","32/32 [==============================] - 38s 1s/step - loss: 0.4743 - accuracy: 0.7817 - val_loss: 0.5074 - val_accuracy: 0.7898\n","Epoch 7/10\n","32/32 [==============================] - 36s 1s/step - loss: 0.3712 - accuracy: 0.8571 - val_loss: 0.3633 - val_accuracy: 0.8636\n","Epoch 8/10\n","32/32 [==============================] - 39s 1s/step - loss: 0.3216 - accuracy: 0.9087 - val_loss: 0.3412 - val_accuracy: 0.8864\n","Epoch 9/10\n","32/32 [==============================] - 40s 1s/step - loss: 0.3199 - accuracy: 0.8730 - val_loss: 0.7535 - val_accuracy: 0.5909\n","Epoch 10/10\n","32/32 [==============================] - 38s 1s/step - loss: 0.3454 - accuracy: 0.8631 - val_loss: 0.3134 - val_accuracy: 0.8523\n"]}],"source":["history = model.fit(\n","    train_data_gen,\n","    steps_per_epoch=total_train // batch_size,\n","    epochs=epochs,\n","    validation_data=val_data_gen,\n","    validation_steps=total_val // batch_size,\n","    workers=4, # num should be 1 or 2 processors less than the total number of process on your machine\n","    callbacks=[tensorboard_callback]\n",")"]},{"cell_type":"markdown","metadata":{"id":"X0UuYU1o_LRi"},"source":["## Take Away\n","\n","The above task is an exercise in using a pre-trained model in the context of **Transfer Learning**. \n","\n","**Transfer Learning** happens when you take a model trained on a data set $A$ and apply it to the data set $B$. You may or may not choose to re-train the model.\n","\n","We loaded in a pre-trained model (meaning the weight values have been optimized in a previous fit) and updated the values of the weights by re-training them. Note that we didn't reset the model weights; we just continued their training on a different dataset, our data set. \n"]},{"cell_type":"markdown","metadata":{"id":"UPzsgS94tCA5"},"source":["-----\n","# Custom CNN Model\n","\n","In this step, write and train your convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. \n","\n","**Protip:** You'll be creating a 2nd instance of this same model in the next section. Instead of copying and pasting all this code, embed it in a `def create_model()` that returns a compiled model. \n","\n","Free to reference the custom CNN model that we built together in the guided project. "]},{"cell_type":"code","execution_count":47,"metadata":{"deletable":false,"id":"hnbJJie3tCA5","nbgrader":{"cell_type":"code","checksum":"4fe146a54f63df2e46fa8396e93b1532","grade":false,"grade_id":"cell-f3d186ae68873264","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def create_model():\n","    \"\"\"\n","    Since we'll be using this model again in the next section, it's useful to create a function \n","    that returns a compiled model.\n","    \"\"\"\n","    # YOUR CODE HERE\n","    ###BEGIN SOLUTION\n","\n","    # keras calls them filters and kernels, we call them weight matrices \n","    # weight matrices have cell values \n","    # weight matrix cell values are randomly initialized \n","    # and get updated during Gradient Descent just like weights in the FCFF network \n","    n_weight_matrices = 32\n","\n","    # specify the window size (i.e. 3 cell high and 3 cell wide)\n","    # convolution kernel, convolution filter, weight matrix\n","    weight_matrix_size = (3,3)\n","\n","    # output of the convolutions between the weight matrices and the image pixels \n","    # will be passed into the activation function (if activate function isn't None)\n","    # ie. y = f(w*x + b)\n","    act_func = 'relu'\n","\n","    # dim of the image: 32 cell high, 32 cell wide, and 3 channels (one for Red, one for Blue, and one for Green)\n","    # this means our images are not matrices, they are tensors \n","    image_dim = (IMG_HEIGHT,IMG_WIDTH,3)  # 224, 224, 3 but the truncated (no padding) result is 222\n","\n","    pool_size = (2,2)\n","\n","    model = Sequential()\n","\n","    ### Note about the weights in the convolutional layers ####\n","    # during training, the weights in the weight matrix (i.e. the windows used for convolutions) are updated \n","    # these weights are responsible for identifying important features in the images (i.e. feature engineering)\n","    # they must be tuned so that each convolutional layer is able to identify features (i.e. feature engineering)\n","    # each layer creates features, then pass those features to the next layer so that the next layer can use those features to create new features (hieratical features!)\n","\n","\n","    # 1st conv layer \n","    model.add(Conv2D(n_weight_matrices, \n","                    weight_matrix_size, \n","                    activation=act_func, \n","                    input_shape=image_dim))\n","\n","    # 1st pooling layer \n","    model.add(MaxPooling2D(pool_size))\n","\n","    # 2nd conv layer \n","    model.add(Conv2D(64,\n","                    weight_matrix_size, \n","                    activation=act_func))\n","\n","    # 2nd pooling layer \n","    model.add(MaxPooling2D(pool_size))\n","\n","    # 3rd conv layer \n","    model.add(Conv2D(62, \n","                    weight_matrix_size, \n","                    activation=act_func))\n","\n","    # not adding a 3rd pooling layer becaue the size of the image at this point\n","    # is already very small, i.e. (4,4) # Actually (52,52) so I did add.\n","\n","    # 3rd pooling layer \n","    model.add(MaxPooling2D(pool_size))\n","\n","    # flatten the image matrix into a row vector \n","    model.add(Flatten())\n","\n","    # hidden layer in FCFF portion of model \n","    model.add(Dense(64, activation=act_func))\n","\n","    # hidden layer in FCFF portion of model \n","    model.add(Dense(1, activation=\"sigmoid\"))\n","\n","    # Compile Model\n","    # the only code that is missing is the compile method\n","    model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n","\n","    return model\n","    ###END SOLUTION"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"tussXeGS_LRi"},"outputs":[],"source":["# instantiate a model \n","model = create_model()"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"VTpyAncr_LRi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 222, 222, 32)      896       \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 111, 111, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 54, 54, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 52, 52, 62)        35774     \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 26, 26, 62)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_1 (Flatten)         (None, 41912)             0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                2682432   \n","                                                                 \n"," dense_5 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,737,663\n","Trainable params: 2,737,663\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"fx3aFuQI_LRi"},"outputs":[],"source":["# include this callback in your fit method\n","# we'll launch tensorboard in the last section\n","logdir = os.path.join(\"logs\", \"baseline_model\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"CwM4GsaetCA_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","32/32 [==============================] - 23s 664ms/step - loss: 0.4452 - accuracy: 0.7817 - val_loss: 0.2097 - val_accuracy: 0.9205\n","Epoch 2/10\n","32/32 [==============================] - 17s 512ms/step - loss: 0.2056 - accuracy: 0.9206 - val_loss: 0.1540 - val_accuracy: 0.9489\n","Epoch 3/10\n","32/32 [==============================] - 17s 514ms/step - loss: 0.1508 - accuracy: 0.9444 - val_loss: 0.1535 - val_accuracy: 0.9318\n","Epoch 4/10\n","32/32 [==============================] - 16s 495ms/step - loss: 0.1471 - accuracy: 0.9365 - val_loss: 0.1319 - val_accuracy: 0.9432\n","Epoch 5/10\n","32/32 [==============================] - 16s 497ms/step - loss: 0.1328 - accuracy: 0.9504 - val_loss: 0.3695 - val_accuracy: 0.8182\n","Epoch 6/10\n","32/32 [==============================] - 18s 566ms/step - loss: 0.1321 - accuracy: 0.9385 - val_loss: 0.1286 - val_accuracy: 0.9545\n","Epoch 7/10\n","32/32 [==============================] - 23s 717ms/step - loss: 0.0862 - accuracy: 0.9663 - val_loss: 0.2086 - val_accuracy: 0.9318\n","Epoch 8/10\n","32/32 [==============================] - 21s 662ms/step - loss: 0.0661 - accuracy: 0.9762 - val_loss: 0.2089 - val_accuracy: 0.9034\n","Epoch 9/10\n","32/32 [==============================] - 21s 659ms/step - loss: 0.0909 - accuracy: 0.9722 - val_loss: 0.4940 - val_accuracy: 0.8239\n","Epoch 10/10\n","32/32 [==============================] - 23s 699ms/step - loss: 0.2018 - accuracy: 0.9226 - val_loss: 0.1789 - val_accuracy: 0.9432\n"]}],"source":["# Fit Model\n","epochs = 10\n","history = model.fit(\n","    train_data_gen, # in place of x, y\n","    steps_per_epoch=total_train // batch_size,\n","    epochs=epochs,\n","    validation_data=val_data_gen,\n","    validation_steps=total_val // batch_size,\n","    workers=7, # num should be 1 or 2 processors less than the total number of process on your machine \n","    callbacks=[tensorboard_callback]\n",")"]},{"cell_type":"markdown","metadata":{"id":"FNTHjUddtCBB"},"source":["------\n","# Custom CNN Model with Image Augmentation\n","\n","To simulate an increase in an image sample, you can apply image manipulation techniques: cropping, rotation, stretching, etc. <br>\n","Luckily, Keras has some handy functions for us to apply these techniques to our mountain and forest example. In addition, you should be able to modify our image generator for the problem. Check out these resources to help you get started: \n","\n","1. [**Keras ImageGenerator Class**](https://keras.io/preprocessing/image/#imagedatagenerator-class) documentation for the tool that we need to use to augment our images\n","2. [**Building a powerful image classifier with very little data**](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) This is essentially a tutorial on using the ImageGenerator class to create augmented images. You can essentially copy and paste the relevant code, though don't do that blindly! \n"," "]},{"cell_type":"markdown","metadata":{"id":"tE6HETY9_LRj"},"source":["### Use ImageDataGenerator to Create Augmented Data\n","\n","Use the parameters for ImageDataGenerator that will enable you to generate an augmented version of images that we already have. Here are some of the relevant parameters to help you get started. \n","\n","- rescale\n","- shear_range\n","- zoom_range\n","- horizontal_flip\n","\n","### Only Create Augmented Images for the Training Data\n","\n","We want to be able to make a comparison with the same CNN model (or models) from above. \n","\n","To do that, we will augment the training data but not the validation data. Then we'll compare the accuracy and loss on the validation set. \n","\n","That way, we are comparing the performance of the same model architecture on the same test set; the only difference will be the augmented training data.Therefore, we'll be in a position to determine if augmenting our training data helped improve our model performance. \n","\n","This is an example of a controlled experiment."]},{"cell_type":"code","execution_count":52,"metadata":{"id":"XKioBv3WtCBB"},"outputs":[],"source":["batch_size = 16\n","\n","# ImageDataGenerator can rescale data from within \n","max_pixel_val = 255.\n","rescale = 1./max_pixel_val"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"c_c1LPMAhNju"},"outputs":[],"source":["# YOUR CODE HERE\n","# create an ImageDataGenerator instance - save result to `train_datagen_aug`\n","train_datagen_aug = ImageDataGenerator(\n","    rescale=rescale,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True\n",")"]},{"cell_type":"code","execution_count":55,"metadata":{"deletable":false,"id":"j58t0ITa_LRk","nbgrader":{"cell_type":"code","checksum":"d40af8e7b09b4fccca8aec84f908e8ff","grade":false,"grade_id":"cell-d458fa433ebcf555","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 520 images belonging to 2 classes.\n"]}],"source":["# call the .flow_from_directory() method - save result to `train_data_gen_aug`\n","# protip: be mindful of the parameters\n","\n","# YOUR CODE HERE\n","train_datagen_aug = train_datagen_aug.flow_from_directory(batch_size=batch_size,\n","                                                          directory=train_dir,\n","                                                          shuffle=True,\n","                                                          target_size=(IMG_HEIGHT, IMG_WIDTH),\n","                                                          class_mode=\"binary\",\n","                                                          color_mode='rgb')"]},{"cell_type":"markdown","metadata":{"id":"oh_nrYbJ_LRk"},"source":["### Augment a Single Image\n","\n","Now that you have instantiated the `ImageDataGenerator` object that created augmented images for the training set.  Let's visual those augmented images to get a sense of what augmented images look like! \n"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"67w2VBka_LRk"},"outputs":[{"data":{"text/plain":["'./data/train/mountain/art1131.jpg'"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["# filename of image that we will augment\n","# this is just one of 100's of training images to choose from\n","# feel free to explore the training data directories and choose another image\n","# this image was selected from the mountain images \n","img_to_aug = \"art1131.jpg\"\n","\n","# replace with YOUR home directory name \n","# home_dir = \"alexanderbarriga\"\n","\n","# create absolute file path to image file\n","path_to_single_img = \"./data/train/mountain/{0}\".format(img_to_aug)\n","\n","path_to_single_img"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"KpPmR6bh_LRk"},"outputs":[],"source":["# load in image from file and reshape\n","img = load_img(path_to_single_img)  \n","x = img_to_array(img) \n","x = x.reshape((1,) + x.shape)  "]},{"cell_type":"markdown","metadata":{"id":"9BfEv1qd_LRk"},"source":["Create a temporary directory to store the augmented images that we will create to visualize. "]},{"cell_type":"code","execution_count":58,"metadata":{"id":"rrSovtiM_LRk"},"outputs":[],"source":["# this is a terminal command that we are running in the notebook by including a `!` \n","# feel free to delete this temp dir after visualizing the aug images below\n","# you only need to create this dir once\n","# !mkdir preview_img"]},{"cell_type":"markdown","metadata":{"id":"xT0U8L4O_LRk"},"source":["Use the training data generator we just created to create 20 augmented images of the same original image."]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["from keras_preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"wHAfcJGz_LRk"},"outputs":[{"ename":"AttributeError","evalue":"'DirectoryIterator' object has no attribute 'flow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb Cell 53'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=4'>5</a>\u001b[0m n_aug_imgs_to_create \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=5'>6</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_datagen_aug\u001b[39m.\u001b[39;49mflow(x, \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=7'>8</a>\u001b[0m                                     batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=8'>9</a>\u001b[0m                                     save_to_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpreview_img\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=9'>10</a>\u001b[0m                                     save_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mart\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=10'>11</a>\u001b[0m                                     save_format\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mjpeg\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=11'>12</a>\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000051vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m n_aug_imgs_to_create:\n","\u001b[0;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'flow'"]}],"source":["# the .flow() command below generates batches of randomly transformed images\n","# and saves the results to the `preview_img` directory\n","\n","# create 20 aug images\n","n_aug_imgs_to_create = 20\n","i = 0\n","for batch in train_datagen_aug.flow(x, \n","                                    batch_size=1,\n","                                    save_to_dir='preview_img', \n","                                    save_prefix='art', \n","                                    save_format='jpeg'):\n","    i += 1\n","    if i > n_aug_imgs_to_create:\n","        break  # otherwise the generator would loop indefinitely"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"QwPQrqsK_LRl"},"outputs":[],"source":["# create list populated with augmented image filenames\n","file_names = [f for f in listdir(\"preview_img\") if isfile(join(\"preview_img\", f))]"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"rzxfeSCt_LRl"},"outputs":[],"source":["# load and prep images into a list \n","aug_imgs = []\n","for filename in file_names:\n","\n","    img = load_img(\"preview_img/{}\".format(filename)) \n","    a_img = img_to_array(img)  \n","    a_img = a_img.reshape((1,) + x.shape) \n","    aug_imgs.append(a_img)"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"2mQ9IjNQ_LRl"},"outputs":[{"ename":"NameError","evalue":"name 'a_img' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000054vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# notice that we are playing with a rank 5 tensor \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000054vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m# we can ignore the first two numbers\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000054vscode-remote?line=2'>3</a>\u001b[0m a_img\u001b[39m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'a_img' is not defined"]}],"source":["# notice that we are playing with a rank 5 tensor \n","# we can ignore the first two numbers\n","a_img.shape"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"l7C5s4r0_LRl"},"outputs":[{"ename":"NameError","evalue":"name 'a_img' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb Cell 57'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000055vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# the last 3 numbers have the actual image data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000055vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m# (num 1, num 2, num 3) = (img height, img width, color channels)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000055vscode-remote?line=2'>3</a>\u001b[0m a_img[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n","\u001b[0;31mNameError\u001b[0m: name 'a_img' is not defined"]}],"source":["# the last 3 numbers have the actual image data\n","# (num 1, num 2, num 3) = (img height, img width, color channels)\n","a_img[0][0].shape"]},{"cell_type":"markdown","metadata":{"id":"wCT4NSnZ_LRl"},"source":["### Visualize Augmented Images\n","\n","Notice that the augmented images are just the original image with slight changes. For example, one image might be flipped to the y-axis or shifted along the x or y-axis, and the right-hand side might be clipped, the image might be scaled up or down, or some combination of changes. "]},{"cell_type":"code","execution_count":72,"metadata":{"id":"qK7U6cxc_LRl"},"outputs":[{"data":{"text/plain":["<Figure size 1440x1440 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(20,20))\n","for i, a_img in enumerate(aug_imgs):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(a_img[0][0]/255.)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NeLpJ-0k_LRl"},"source":["Now the real question is, does any of this ultimately matter? For example, do these changes help our model's ability to learn and generalize better? Well, let's go ahead and run that experiment. \n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"5n5W2rtg_LRl"},"source":["## Re-train Your Custom CNN Model Using the Augmented Dataset\n","\n","We have created a data generator that creates augmented versions of the training images (and not the validation images). Thus, we can create a new instance of our custom CNN model with the same architecture, same parameters such as batch size and epochs, and see if augmented data helps. "]},{"cell_type":"code","execution_count":73,"metadata":{"id":"XrWRmzT-_LRm"},"outputs":[],"source":["aug_model = create_model()"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"8oG2LHML_LRm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_6 (Conv2D)           (None, 222, 222, 32)      896       \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 111, 111, 32)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 109, 109, 64)      18496     \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 54, 54, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 52, 52, 62)        35774     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 26, 26, 62)       0         \n"," 2D)                                                             \n","                                                                 \n"," flatten_2 (Flatten)         (None, 41912)             0         \n","                                                                 \n"," dense_6 (Dense)             (None, 64)                2682432   \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,737,663\n","Trainable params: 2,737,663\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["aug_model.summary()"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"k_mI7_w5_LRm"},"outputs":[],"source":["logdir = os.path.join(\"logs\", \"aug_model\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"_pWNYL0j_LRm"},"outputs":[{"ename":"NameError","evalue":"name 'train_data_gen_aug' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb Cell 65'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Fit Model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=4'>5</a>\u001b[0m history \u001b[39m=\u001b[39m aug_model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=5'>6</a>\u001b[0m     train_data_gen_aug,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=6'>7</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39mtotal_train \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=8'>9</a>\u001b[0m     validation_data\u001b[39m=\u001b[39mval_data_gen,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=9'>10</a>\u001b[0m     validation_steps\u001b[39m=\u001b[39mtotal_val \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=10'>11</a>\u001b[0m     workers\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m# num should be 1 or 2 processors less than the total number of process on your machine \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=11'>12</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[tensorboard_callback]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/greg/github/DS-Unit-4-Sprint-3-Deep-Learning/module2-convolutional-neural-networks/DS_432_Convolution_Neural_Networks_Assignment.ipynb#ch0000063vscode-remote?line=12'>13</a>\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'train_data_gen_aug' is not defined"]}],"source":["# Fit Model\n","\n","epochs = 10\n","\n","history = aug_model.fit(\n","    train_data_gen_aug,\n","    steps_per_epoch=total_train // batch_size,\n","    epochs=epochs,\n","    validation_data=val_data_gen,\n","    validation_steps=total_val // batch_size,\n","    workers=10, # num should be 1 or 2 processors less than the total number of process on your machine \n","    callbacks=[tensorboard_callback]\n",")"]},{"cell_type":"markdown","metadata":{"id":"H9wkCHW9_LRm"},"source":["-----\n","\n","# Compare Model Results"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"FDeoXYRL_LRm"},"outputs":[{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 20811), started 0:02:34 ago. (Use '!kill 20811' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-bb3b7931db57fcb4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-bb3b7931db57fcb4\");\n","          const url = new URL(\"http://localhost\");\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%tensorboard --logdir logs"]},{"cell_type":"markdown","metadata":{"id":"UXUjtO2Y_LRm"},"source":["------\n","\n","### Time for Questions \n","\n","Take a look at the `epoch_accuracy` plot and answer the following questions. \n","\n","Optionally move the `smoothing` slider all the way to zero to view the raw scores. \n","\n","By the way, your results may look different than your classmates depending on how you choose to build your custom CNN model. \n","\n","\n","**Question 1:** Which of the three models performed the best? "]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"nIdpawh5_LRm","nbgrader":{"cell_type":"markdown","checksum":"c582761ccc143c5d248d27ce49160bb8","grade":true,"grade_id":"cell-0e8a26fd93ff75d1","locked":false,"points":0,"schema_version":3,"solution":true,"task":false}},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"lJ_MqWk9_LRn"},"source":["**Question 2:** Did augmenting the training data help our custom CNN model improve its score? If so, why, if not, why not?"]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"2zGjjIKI_LRn","nbgrader":{"cell_type":"markdown","checksum":"36700b3583a95d15b36e934bf8e61b02","grade":true,"grade_id":"cell-05178550630857e1","locked":false,"points":0,"schema_version":3,"solution":true,"task":false}},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"A5MtFuoC_LRn"},"source":["**Question 3:** Could one or more of the three models benefit from training on more than ten epochs? If so, why, if not, why not?"]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"ShzvLy0r_LRn","nbgrader":{"cell_type":"markdown","checksum":"5739b5ef24367658384ddbb226ab6756","grade":true,"grade_id":"cell-ce4cf1e6eb96f202","locked":false,"points":0,"schema_version":3,"solution":true,"task":false}},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"qC-p-XRL_LRn"},"source":["**Question 4:** If you didn't use regularization for your custom CNN, do you think the baseline model and the aug model could improve their scores if regularization was used? If so, why, if not, why not?\n","\n","Consider reviewing your Sprint 2 Module Assignment 2 experimental results on regularization. "]},{"cell_type":"markdown","metadata":{"deletable":false,"id":"UyMb2ldO_LRn","nbgrader":{"cell_type":"markdown","checksum":"963bdf6b8a29e4d8166bef574ebe66d9","grade":true,"grade_id":"cell-9d8c200f58160233","locked":false,"points":0,"schema_version":3,"solution":true,"task":false}},"source":["YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"cizjQvHZ_LRn"},"source":["-----"]},{"cell_type":"markdown","metadata":{"id":"uT3UV3gap9H6"},"source":["# Resources and Stretch Goals\n","\n","Stretch goals\n","- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g., download images of parties, recognize all that contain balloons)\n","- Check out [other available pre-trained networks](https://tfhub.dev), try some, and compare\n","- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g., [Visual Genome](https://visualgenome.org/)) on the topic\n","- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n","- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed into your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n","\n","Resources\n","- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n","- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution-based object detection system, focused on inference speed (for applications too e.g., self-driving vehicles)\n","- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n","- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n","- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"DS_432_Convolution_Neural_Networks_Assignment.ipynb","provenance":[{"file_id":"https://github.com/ryanleeallred/DS-Unit-4-Sprint-3-Deep-Learning/blob/main/module2-convolutional-neural-networks/LS_DS_432_Convolution_Neural_Networks_Assignment.ipynb","timestamp":1637632238626}]},"interpreter":{"hash":"bd8e00f63139cf9feba2231e80738099b1c7a506243ed7a456c0669dfb5d242f"},"kernelspec":{"display_name":"py37  (Python3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"nteract":{"version":"0.23.1"}},"nbformat":4,"nbformat_minor":0}
